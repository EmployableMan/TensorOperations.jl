<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Implementation · TensorOperations.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>TensorOperations.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><span class="toctext">Home</span><ul><li><a class="toctext" href="../">TensorOperations.jl</a></li><li><a class="toctext" href="../indexnotation/">Index notation with <code>@tensor</code> macro</a></li><li><a class="toctext" href="../functions/">Functions</a></li><li><a class="toctext" href="../cache/">Cache for temporaries</a></li><li class="current"><a class="toctext" href>Implementation</a><ul class="internal"><li><a class="toctext" href="#Building-blocks-1">Building blocks</a></li><li><a class="toctext" href="#Index-notation-and-the-@tensor-macro-1">Index notation and the <code>@tensor</code> macro</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Home</li><li><a href>Implementation</a></li></ul><a class="edit-page" href="https://github.com/Jutho/TensorOperations.jl/blob/master/docs/src/implementation.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Implementation</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Implementation-1" href="#Implementation-1">Implementation</a></h1><h2><a class="nav-anchor" id="Building-blocks-1" href="#Building-blocks-1">Building blocks</a></h2><p>Under the hood, the implementation is  centered around the primitive operations: addition, tracing and contraction. These operations are implemented for arbitrary strided arrays from Julia Base, i.e. <code>Array</code>s, views with ranges thereof, and certain reshape operations. This includes certain arrays that can only be determined to be strided on runtime, and does therefore not coincide with the type union <code>StridedArray</code> from Julia Base. In fact, the methods accept <code>AbstractArray</code> objects, but convert these to <code>(Unsafe)StridedView</code> objects from the package <a href="https://github.com/Jutho/Strided.jl">Strided.jl</a>, and we refer to this package for a more detailed discussion of which arrays are supported and why.</p><p>Nonetheless, the implementation can easily be extended to user defined types, especially if they just wrap multidimensional data with a strided memory storage. The building blocks resemble the functions discussed above, but have a different interface and are more general. They are used both by the functions as well as by the <code>@tensor</code> macro, as discussed below. Note that these functions are not exported.</p><ul><li><p><code>add!(α, A, conjA, β, C, indCinA)</code></p><p>Implements <code>C = β*C+α*permute(op(A))</code> where <code>A</code> is permuted according to <code>indCinA</code> and <code>op</code> is <code>conj</code> if <code>conjA=Val{:C}</code> or the identity map if <code>conjA=Val{:N}</code>. The indexable collection <code>indCinA</code> contains as nth entry the dimension of <code>A</code> associated with the nth dimension of <code>C</code>.</p></li><li><p><code>trace!(α, A, conjA, β, C, indCinA, cindA1, cindA2)</code></p><p>Implements <code>C = β*C+α*partialtrace(op(A))</code> where <code>A</code> is permuted and partially traced, according to <code>indCinA</code>, <code>cindA1</code> and <code>cindA2</code>, and <code>op</code> is <code>conj</code> if <code>conjA=Val{:C}</code> or the identity map if <code>conjA=Val{:N}</code>. The indexable collection <code>indCinA</code> contains as nth entry the dimension of <code>A</code> associated with the nth dimension of <code>C</code>. The partial trace is performed by contracting dimension <code>cindA1[i]</code> of <code>A</code> with dimension <code>cindA2[i]</code> of <code>A</code> for all <code>i in 1:length(cindA1)</code>.</p></li><li><p><code>contract!(α, A, conjA, B, conjB, β, C, oindA, cindA, oindB, cindB, indCinoAB, [method])</code></p><p>Implements <code>C = β*C+α*contract(op(A),op(B))</code> where <code>A</code> and <code>B</code> are contracted according to <code>oindA</code>, <code>cindA</code>, <code>oindB</code>, <code>cindB</code> and <code>indCinoAB</code>. The operation <code>op</code> acts as <code>conj</code> if <code>conjA</code> or <code>conjB</code> equal <code>Val{:C}</code> or as the identity map if <code>conjA</code> (<code>conjB</code>) equal <code>Val{:N}</code>. The dimension <code>cindA[i]</code> of <code>A</code> is contracted with dimension <code>cindB[i]</code> of <code>B</code>. The <code>n</code>th dimension of C is associated with an uncontracted (open) dimension of <code>A</code> or <code>B</code> according to <code>indCinoAB[n] &lt; NoA ? oindA[indCinoAB[n]] : oindB[indCinoAB[n]-NoA]</code> with <code>NoA=length(oindA)</code> the number of open dimensions of <code>A</code>.</p><p>The optional argument <code>method</code> specifies whether the contraction is performed using BLAS matrix multiplication by specifying <code>Val{:BLAS}</code>, or using a native algorithm by specifying <code>Val{:native}</code>. The native algorithm does not copy the data but is typically slower. The BLAS-based algorithm is chosen by default, if the element type of the output array is in <code>Base.LinAlg.BlasFloat</code>.</p></li></ul><h2><a class="nav-anchor" id="Index-notation-and-the-@tensor-macro-1" href="#Index-notation-and-the-@tensor-macro-1">Index notation and the <code>@tensor</code> macro</a></h2><footer><hr/><a class="previous" href="../cache/"><span class="direction">Previous</span><span class="title">Cache for temporaries</span></a></footer></article></body></html>
